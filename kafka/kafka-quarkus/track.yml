slug: kafka-kafka-quarkus
id: trcegnjbjtej
type: track
title: Apache Kafka with Reactive Messaging
description: |
  In this scenario, you will create a Quarkus application that uses the [MicroProfile Reactive Messaging](https://download.eclipse.org/microprofile/microprofile-reactive-messaging-1.0/microprofile-reactive-messaging-spec.pdf) extension to send events to Apache Kafka.

  Kafka is generally used for two broad classes of applications:

  - Building real-time streaming data pipelines that reliably get data between systems or applications
  - Building real-time streaming applications that transform or react to the streams of data

  The Quarkus extension uses [SmallRye Reactive Messaging](https://smallrye.io/smallrye-reactive-messaging/smallrye-reactive-messaging/2/index.html) to implement the connectors to Kafka. SmallRye is a framework for building event-driven, data streaming, and event-sourcing applications using [Context and Dependency Injection](http://www.cdi-spec.org/) (CDI) for Java.

  ## Channels and Streams

  When dealing with an event-driven or data streaming application, there are a few concepts and terms we need to understand.

  In the application, `messages` flow on a _channel_. A _channel_ is a virtual destination identified by a name. SmallRye connects the component to a channel they read and to a channel they populate. The resulting structure is a stream: Messages flow between components through channels.

  ## Connectors

  An application interacts with an event broker, which transmits messages using _connectors_. A _connector_ is a piece of code that connects to a broker to:

  1. Receive messages from the event broker and propagate them to the application
  2. Send messages provided by the application to the broker

  To achieve this, connectors are configured to map incoming messages to a specific *channel* (consumed by the application), and to collect outgoing messages sent to a specific channel by the application.

  Each connector has a name. This name is referenced by the application to indicate that a specific channel is managed by this connector.

  ## Apache Kafka Connector

  A Kafka connector adds support for Kafka to SmallRye. With it you can receive Kafka Records as well as write `message` into Kafka.

  The Kafka Connector is based on the [Vert.x Kafka Client](https://vertx.io/docs/vertx-kafka-client/java/).
icon: https://logodix.com/logo/1910931.png
level: beginner
tags:
- openshift
owner: openshift
developers:
- nvinto@redhat.com
- rjarvine@redhat.com
- dahmed@redhat.com
private: false
published: true
challenges:
- slug: step1
  id: qamvzlx2smer
  type: challenge
  title: Adding a Quarkus Reactive Messaging Extension
  notes:
  - type: text
    contents: |
      In this scenario, you will create a Quarkus application that uses the [MicroProfile Reactive Messaging](https://download.eclipse.org/microprofile/microprofile-reactive-messaging-1.0/microprofile-reactive-messaging-spec.pdf) extension to send events to Apache Kafka.

      Kafka is generally used for two broad classes of applications:

      - Building real-time streaming data pipelines that reliably get data between systems or applications
      - Building real-time streaming applications that transform or react to the streams of data

      The Quarkus extension uses [SmallRye Reactive Messaging](https://smallrye.io/smallrye-reactive-messaging/smallrye-reactive-messaging/2/index.html) to implement the connectors to Kafka. SmallRye is a framework for building event-driven, data streaming, and event-sourcing applications using [Context and Dependency Injection](http://www.cdi-spec.org/) (CDI) for Java.

      ## Channels and Streams

      When dealing with an event-driven or data streaming application, there are a few concepts and terms we need to understand.

      In the application, `messages` flow on a _channel_. A _channel_ is a virtual destination identified by a name. SmallRye connects the component to a channel they read and to a channel they populate. The resulting structure is a stream: Messages flow between components through channels.

      ## Connectors

      An application interacts with an event broker, which transmits messages using _connectors_. A _connector_ is a piece of code that connects to a broker to:

      1. Receive messages from the event broker and propagate them to the application
      2. Send messages provided by the application to the broker

      To achieve this, connectors are configured to map incoming messages to a specific *channel* (consumed by the application), and to collect outgoing messages sent to a specific channel by the application.

      Each connector has a name. This name is referenced by the application to indicate that a specific channel is managed by this connector.

      ## Apache Kafka Connector

      A Kafka connector adds support for Kafka to SmallRye. With it you can receive Kafka Records as well as write `message` into Kafka.

      The Kafka Connector is based on the [Vert.x Kafka Client](https://vertx.io/docs/vertx-kafka-client/java/).
  assignment: |
    You start this scenario with a basic Maven-based application, which is created using the Quarkus maven plugin.

    ### Add an extension to integrate with Kafka

    The current project needs the extensions to be added to integrate Quarkus with Apache Kafka.

    Change to the project folder:

    ```
    cd /opt/projects/kafka-quarkus
    ```

    Install the extension into the project with the following command:

    ```
    mvn quarkus:add-extension -Dextension="quarkus-smallrye-reactive-messaging-kafka"
    ```

    >The first time you add the extension will take longer, as Maven downloads new dependencies.

    This will add the necessary entries in your `pom.xml` to bring in the Kafka extension. You should see a fragment similar to this around line 50:

    ```xml
    ...
    <dependency>
        <groupId>io.quarkus</groupId>
        <artifactId>quarkus-smallrye-reactive-messaging-kafka</artifactId>
    </dependency>
    ...
    ```

    ### Configure a channel to integrate with the event broker

    Next, we need to configure the application to define how are we going to connect to the event broker.

    The MicroProfile Reactive Messaging properties are structured as follows:

    ```properties
    mp.messaging.[outgoing|incoming].{channel-name}.property=value
    ```

    The `channel-name` segment must match the value set in the `@Incoming` and `@Outgoing` annotations. To indicate that a channel is managed by the Kafka connector we need:

    ```properties
    mp.messaging.[outgoing|incoming].{channel-name}.connector=smallrye-kafka
    ```

    Open the `src/main/resources/application.properties` file to add the following configuration:

    <pre class="file" data-filename="./src/main/resources/application.properties" data-target="replace">
    # Configuration file
    kafka.bootstrap.servers=my-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092

    mp.messaging.outgoing.uber.connector=smallrye-kafka
    mp.messaging.outgoing.uber.key.serializer=org.apache.kafka.common.serialization.StringSerializer
    mp.messaging.outgoing.uber.value.serializer=org.apache.kafka.common.serialization.StringSerializer
    </pre>

    > You can click **Copy to Editor** to add the values into the file

    You can see we added the kafka bootstrap server hostname and port for the broker locations and the configuration for a channel named `uber`. The `key` and `value` serializers are part of the  [Producer configuration](https://kafka.apache.org/documentation/#producerconfigs) and [Consumer configuration](https://kafka.apache.org/documentation/#consumerconfigs) to encode the message payload.

    >You donâ€™t need to set the Kafka topic. By default, it uses the channel name (`uber`). You can, however, configure the topic attribute to override it.
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 75
- slug: step2
  id: djq7ycrosq8o
  type: challenge
  title: Creating an event generator
  assignment: |
    This project has already a `VehicleGenerator` class that will be used to send events to Kafka. However, it is missing the Reactive Messaging code.

    ### Writing Kafka Records

    The Kafka connector can write Reactive Messaging messages as Kafka records.

    Open the `src/main/java/com/redhat/katacoda/kafka/VehicleGenerator.java` file to check the code.

    We will be sending the events to the `uber` channel by adding the following method:

    <pre class="file" data-filename="./src/main/java/com/redhat/katacoda/kafka/VehicleGenerator.java" data-target="insert" data-marker="//  TODO-publisher">
        @Outgoing("uber")
        public Flowable&lt;KafkaRecord&lt;String, String&gt;&gt; generateUber()
        {
            return Flowable.interval(5000, TimeUnit.MILLISECONDS)
                    .map(tick -> {
                        VehicleInfo vehicle = randomVehicle("uber");
                        LOG.info("dispatching vehicle: {}", vehicle);
                        return KafkaRecord.of(String.valueOf(vehicle.getVehicleId()), Json.encodePrettily(vehicle));
                    });
        }
    </pre>

    > You can click in **Copy to Editor** to add the values into the file

    This simple method:

    - Instructs Reactive Messaging to dispatch the items from returned stream to the `uber`channel.
    - Returns an [RX Java 2 stream](https://github.com/ReactiveX/RxJava) (`Flowable`) emitting random vehicle information every 5 seconds
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 75
- slug: step3
  id: ghg8mlfch0ml
  type: challenge
  title: Deploying the application to OpenShift
  assignment: |
    With Quarkus, you can automatically generate OpenShift resources from default and user-supplied configuration. The OpenShift extension is a wrapper extension that brings together the [kubernetes](https://quarkus.io/guides/deploying-to-kubernetes) and [container-image-s2i](https://quarkus.io/guides/container-image#s2i) extensions with useful defaults so that itâ€™s easier to get started with Quarkus on OpenShift.

    Run the following command to add the extension to the project:

    ```
    mvn quarkus:add-extension -Dextensions="openshift"
    ```

    Reopen the `src/main/resources/application.properties` file and click **Copy to Editor** to add the following values:

    <pre class="file" data-filename="./src/main/resources/application.properties" data-target="append">
    # Configures the OpenShift extension options
    quarkus.kubernetes-client.trust-certs=true
    quarkus.container-image.build=true
    quarkus.kubernetes.deploy=true
    quarkus.kubernetes.deployment-target=openshift
    quarkus.openshift.labels.app.openshift.io/runtime=quarkus
    quarkus.s2i.base-jvm-image=registry.access.redhat.com/ubi8/openjdk-8
    </pre>

    Here's a summary of of the properties added:

    * `quarkus.kubernetes-client.trust-certs=true` - We are using self-signed certificates in this example, so this simply says to the extension to trust them.
    * `quarkus.container-image.build=true` - Instructs the extension to build a container image.
    * `quarkus.kubernetes.deploy=true` - Instructs the extension to deploy to OpenShift after the container image is built.
    * `quarkus.kubernetes.deployment-target=openshift` - Instructs the extension to generate and create the OpenShift resources (like `DeploymentConfig`s and `Service`s) after building the container.
    * `quarkus.openshift.expose=true` - Instructs the extension to generate an OpenShift `Route`.
    * `quarkus.openshift.labels.app.openshift.io/runtime=java` - Adds an icon for the application when viewing the application from the Topology view of the OpenShift Developer perspective.

    ### Login to OpenShift

    We'll deploy the application as the `developer` user. Run the following command to login with the OpenShift `oc` CLI tool:

    ```
    oc login -u developer -p developer
    ```

    You should see

    ```sh
    Login successful.

    You have one project on this server: "kafka"

    Using project "kafka".
    ```

    ### Deploy the application to OpenShift

    Now let's deploy the application itself. Run the following command which will build and deploy the application using the OpenShift extension:

    ```
    mvn clean package
    ```

    At the end you should see an output similar to the folllowing:

    ```sh
    ...
    [INFO] [io.quarkus.container.image.openshift.deployment.OpenshiftProcessor] Push successful
    [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Deploying to openshift server: https://openshift:6443/ in namespace: kafka.
    [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Applied: Service kafka-quarkus.
    [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Applied: ImageStream kafka-quarkus.
    [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Applied: ImageStream openjdk-11.
    [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Applied: BuildConfig kafka-quarkus.
    [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Applied: DeploymentConfig kafka-quarkus.
    [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Applied: Route kafka-quarkus.
    [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] The deployed application can be accessed at: http://kafka-quarkus-kafka.2886795273-80-host19nc.environments.katacoda.com
    [INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 87529ms
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD SUCCESS
    [INFO] ------------------------------------------------------------------------
    ```

    >The process will take a few moments to complete.

    We are now ready to check the Kafka records.
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 75
- slug: step4
  id: 3jh20xkqze14
  type: challenge
  title: Checking Kafka records
  assignment: |
    The application will now generate a new event with vehicle information every 5 seconds.

    ### Wait for the application deployment

    The deployment will start the application pods. You can monitor the pods with the following command:

    ```
    oc get pods -l deploymentconfig=kafka-quarkus -w
    ```

    You will see the pod changing the status to `running`:

    ```sh
    NAME                    READY   STATUS    RESTARTS   AGE
    kafka-quarkus-1-mg8cv   0/1     Pending   0          0s
    kafka-quarkus-1-mg8cv   0/1     Pending   0          0s
    kafka-quarkus-1-mg8cv   0/1     ContainerCreating   0          0s
    kafka-quarkus-1-mg8cv   0/1     ContainerCreating   0          2s
    kafka-quarkus-1-mg8cv   0/1     ContainerCreating   0          8s
    kafka-quarkus-1-mg8cv   1/1     Running             0          13s
    ```

    Press <kbd>Ctrl</kbd>+<kbd>C</kbd> to stop the process.

    `^C`{{execute ctrl-seq}}

    # Check the application log

    To verify there was no problem with the application, we can check the logs with the following command:

    ``oc logs dc/kafka-quarkus -f``{{execute interrupt}}

    You should see the information of the Quarkus application connecting to Kafka as well as the output of the sent events:

    ```sh
    ...
    INFO [io.quarkus] (main) kafka-quarkus 1.0.0 on JVM (powered by Quarkus 1.10.3.Final) started in 2.394s. Listening on: http://0.0.0.0:8080
    INFO [io.quarkus] (main) Profile prod activated.
    INFO [io.quarkus] (main) Installed features: [cdi, kubernetes, mutiny, smallrye-context-propagation, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, vertx]
    INFO [com.red.kat.kaf.VehicleGenerator] (RxComputationThreadPool-1) dispatching vehicle: VehicleInfo{provider='uber', vehicleId=1, pricePerMinute=9.633610913249816, timeToPickup=4, availableSpace=2, available=true}
    WARN [org.apa.kaf.cli.NetworkClient] (kafka-producer-network-thread | kafka-producer-uber) [Producer clientId=kafka-producer-uber] Error while fetching metadata with correlation id 3 : {uber=LEADER_NOT_AVAILABLE}
    WARN [org.apa.kaf.cli.NetworkClient] (kafka-producer-network-thread | kafka-producer-uber) [Producer clientId=kafka-producer-uber] Error while fetching metadata with correlation id 4 : {uber=LEADER_NOT_AVAILABLE}
    WARN [org.apa.kaf.cli.NetworkClient] (kafka-producer-network-thread | kafka-producer-uber) [Producer clientId=kafka-producer-uber] Error while fetching metadata with correlation id 5 : {uber=LEADER_NOT_AVAILABLE}
    INFO [com.red.kat.kaf.VehicleGenerator] (RxComputationThreadPool-1) dispatching vehicle: VehicleInfo{provider='uber', vehicleId=2, pricePerMinute=4.502112907514153, timeToPickup=15, availableSpace=5, available=true}
    ```

    Then you will see new events every few seconds.

    Press <kbd>Ctrl</kbd>+<kbd>C</kbd> to stop the process.

    `^C`{{execute ctrl-seq}}

    ### Check the Kafka records

    Now, let see how the messages look within Kafka.

    Check the messages with this command:

    ``oc exec -c kafka my-cluster-kafka-0 -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic uber --from-beginning | jq``{{execute interrupt}}

    The output presents like this:

    ```json
    ...
    {
      "vehicleId": 93,
      "pricePerMinute": 9.57522356183256,
      "timeToPickup": 15,
      "availableSpace": 3,
      "available": true
    }
    {
      "vehicleId": 94,
      "pricePerMinute": 7.255313141956367,
      "timeToPickup": 21,
      "availableSpace": 8,
      "available": true
    }
    ```

    Press <kbd>Ctrl</kbd>+<kbd>C</kbd> to stop the process.

    `^C`{{execute ctrl-seq}}

    Congratulations! You are now sending events to Kafka using the Quarkus Reactive Messaging extension for Kafka.
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 75
checksum: "8786554612597579282"
