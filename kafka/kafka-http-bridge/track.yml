slug: kafka-kafka-http-bridge
id: adgbumk8xnkr
type: track
title: Exposing Apache Kafka through the HTTP Bridge
description: |
  Apache Kafka uses a custom protocol on top of TCP/IP for communication between applications and the Kafka cluster. Clients are supported in many different programming languages, but there are certain scenarios where it is not possible to use such clients. In this situation, you can use the standard HTTP/1.1 protocol to access Kafka instead.

  The Red Hat AMQ Streams Kafka Bridge provides an API for integrating HTTP-based clients with a Kafka cluster running on AMQ Streams. Applications can perform typical operations such as:

  * Sending messages to topics
  * Subscribing to one or more topics
  * Receiving messages from the subscribed topics
  * Committing offsets related to the received messages
  * Seeking to a specific position

  As with AMQ Streams, the Kafka Bridge is deployed into an OpenShift cluster using the AMQ Streams Cluster Operator, or installed on Red Hat Enterprise Linux using downloaded files.

  ![HTTP integration](https://access.redhat.com/webassets/avalon/d/Red_Hat_AMQ-7.7-Using_AMQ_Streams_on_OpenShift-en-US/images/750556a6bc4af4daeca4b1df0fd24835/kafka-bridge.png)

  In the following tutorial, you will deploy the Kafka Bridge and use it to connect to your Apache Kafka cluster using HTTP.
icon: https://logodix.com/logo/1910931.png
level: intermediate
tags:
- openshift
owner: openshift
developers:
- nvinto@redhat.com
- rjarvine@redhat.com
- dahmed@redhat.com
private: false
published: true
challenges:
- slug: step1
  id: zn6xkmasyxst
  type: challenge
  title: Deploying the HTTP Bridge
  notes:
  - type: text
    contents: |
      Apache Kafka uses a custom protocol on top of TCP/IP for communication between applications and the Kafka cluster. Clients are supported in many different programming languages, but there are certain scenarios where it is not possible to use such clients. In this situation, you can use the standard HTTP/1.1 protocol to access Kafka instead.

      The Red Hat AMQ Streams Kafka Bridge provides an API for integrating HTTP-based clients with a Kafka cluster running on AMQ Streams. Applications can perform typical operations such as:

      * Sending messages to topics
      * Subscribing to one or more topics
      * Receiving messages from the subscribed topics
      * Committing offsets related to the received messages
      * Seeking to a specific position

      As with AMQ Streams, the Kafka Bridge is deployed into an OpenShift cluster using the AMQ Streams Cluster Operator, or installed on Red Hat Enterprise Linux using downloaded files.

      ![HTTP integration](https://access.redhat.com/webassets/avalon/d/Red_Hat_AMQ-7.7-Using_AMQ_Streams_on_OpenShift-en-US/images/750556a6bc4af4daeca4b1df0fd24835/kafka-bridge.png)

      In the following tutorial, you will deploy the Kafka Bridge and use it to connect to your Apache Kafka cluster using HTTP.
  assignment: |2

    Deploying the bridge on OpenShift is really easy using the new `KafkaBridge` custom resource provided by the Red Hat AMQ Streams Cluster Operator.

    ### Logging in to the Cluster using the OpenShift CLI

    Before creating any applications, log in as admin. This is required if you want to log in to the web console and use it.

    To log in to the OpenShift cluster from the _Terminal_ run:

    ```
    oc login -u developer -p developer
    ```

    This will log you in using the credentials:

    * **Username:** ``developer``
    * **Password:** ``developer``

    Use the same credentials to log in to the web console.

    ### Switch your own namespace

    Switch to the (project) namespace called ``kafka`` where the Cluster Operator manages the Kafka resources:

    ```
    oc project kafka
    ```

    ### Deploying Kafka Bridge to your OpenShift cluster

    The deployment uses a YAML file to provide the specification to create a `KafkaBridge` resource.

    Click the link below to open the custom resource (CR) definition for the bridge:

    * `kafka-bridge.yaml`

    The bridge has to connect to the Apache Kafka cluster. This is specified in the `bootstrapServers` property. The bridge then uses a native Apache Kafka consumer and producer for interacting with the cluster.

    >For information about configuring the KafkaBridge resource, see [Kafka Bridge configuration](https://access.redhat.com/documentation/en-us/red_hat_amq/2020.q4/html-single/using_amq_streams_on_openshift/index#assembly-config-kafka-bridge-str).

    Deploy the Kafka Bridge with the custom image:

    ``oc -n kafka apply -f /root/projects/http-bridge/kafka-bridge.yaml``{{execute interrupt}}

    The Kafka Bridge node should be deployed after a few moments. To watch the pods status run the following command:

    ```
    oc get pods -w -l app.kubernetes.io/name=kafka-bridge
    ```

    You will see the pods changing the status to `running`. It should look similar to the following:

    ```bash
    NAME                                READY   STATUS              RESTARTS   AGE
    my-bridge-bridge-6b6d9f785c-dp6nk   0/1     ContainerCreating   0          5s
    my-bridge-bridge-6b6d9f785c-dp6nk   0/1     ContainerCreating   0          12s
    my-bridge-bridge-6b6d9f785c-dp6nk   0/1     Running             0          27s
    my-bridge-bridge-6b6d9f785c-dp6nk   1/1     Running             0          45s
    ```

    > This step might take a couple minutes.

    Press <kbd>Ctrl</kbd>+<kbd>C</kbd> to stop the process.

    `^C`{{execute ctrl-seq}}

    ### Creating an OpenShift route

    After deployment, the Kafka Bridge can only be accessed by applications running in the same OpenShift cluster. If you want to make the Kafka Bridge accessible to applications running outside of the OpenShift cluster, you can expose it manually by using one of the following features:

    * Services of types LoadBalancer or NodePort
    * Kubernetes Ingress
    * OpenShift Routes

    An OpenShift `route` is an OpenShift resource for allowing external access through HTTP/HTTPS to internal services such as the Kafka bridge. We will use this approach for our example.

    Run the following command to expose the bridge service as an OpenShift route:

    ``oc expose svc my-bridge-bridge-service``{{execute interrupt}}

    When the route is created, the Kafka Bridge is reachable through the `https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com` host. You can now use any HTTP client to interact with the REST API exposed by the bridge to send and receive messages without needing to use the native Kafka protocol.
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: intermediate
  timelimit: 225
- slug: step2
  id: clyajpizcjq7
  type: challenge
  title: Producing messages
  assignment: |
    To verify that the Ingress is working properly, try to access the ``/healthy` endpoint of the bridge with the following curl command:

    ``curl -ik https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/healthy``{{execute interrupt}}

    If the bridge is reachable through the OpenShift route, it will return an HTTP response with `200 OK` HTTP code, but an empty body.

    It should look similar to the following example of the output:

    ```sh
    HTTP/2 200
    server: nginx/1.15.0
    date: Tue, 01 Dec 2020 15:01:22 GMT
    content-length: 0
    cache-control: private
    set-cookie: 93b1d08256cbf837e3463c0bba903028=0e558f788ca3bde0c6204c8d9bc783e0; Path=/; HttpOnly; Secure; SameSite=None
    via: 1.1 google
    alt-svc: clear
    ```

    ### Producing messages

    The Kafka cluster we are working with has topic auto-creation enabled, so we can immediately start to send messages through the `/topics/my-topic` endpoint exposed by the HTTP bridge.

    The bridge exposes two main REST endpoints in order to send messages:

    * /topics/{topicname}
    * /topics/{topicname}/partitions/{partitionid}

    The first endpoint sends a message to a topic `topicname`. The second endpoint allows the user to specify the partition using a `partitionid`. Actually, even using the first endpoint we can specify the destination partition in the body of the message.

    The HTTP request payload is always in JSON format, but the message values can be JSON or binary (encoded in base64 because we are sending binary data in a JSON payload so encoding in a string format is needed).

    When performing producer operations, `POST` requests must provide `Content-Type` headers specifying the desired _embedded data format_, either as `json` or `binary`. In this scenario we will be using the **JSON** format.

    Let's send a couple of messages to the `my-topic` topic:

    ```
    curl -s -k -X POST https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/topics/my-topic -H 'content-type: application/vnd.kafka.json.v2+json' -d '{ "records": [ {"key": "key-1","value": "sales-lead-0001"}, {"key": "key-2","value": "sales-lead-0002"} ] }' | jq
    ```

    If the request is successful, the Kafka Bridge returns a `200 OK` HTTP code, with a JSON payload describing the `offsets` array, and the partition and offsets in which the messages are written.

    In this case, the auto-created topic has just one partition, so the response will look something like this:

    ```json
    {
       "offsets":[
          {
             "partition":0,
             "offset":0
          },
          {
             "partition":0,
             "offset":1
          }
       ]
    }
    ```

    Excellent! You have sent your first messages to your Kafka topic through the Kafka Bridge. Now you are ready to start consuming those messages.
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: intermediate
  timelimit: 225
- slug: step3
  id: 62txlwqb7avl
  type: challenge
  title: Subscribing to the topic
  assignment: |
    Before you can perform any consumer operations in the Kafka cluster, you must first create a consumer by using the consumers endpoint.

    You create a consumer through the `/consumers/{groupid}` endpoint by sending an HTTP POST with a body containing some of the supported configuration parameters, the name of the consumer and the data format.

    ### Creating a consumer

    We are going to create a Kafka Bridge consumer named `my-consumer` that will join the consumer group `my-group`.

    Execute the following command to create the consumer:

    ```
    curl -s -k -X POST https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/consumers/my-group -H 'content-type: application/vnd.kafka.v2+json' -d '{"name": "my-consumer","format": "json","auto.offset.reset": "earliest","fetch.min.bytes": 512,"enable.auto.commit": false}' | jq
    ```

    This will create a Kafka consumer connected to the Kafka cluster. If the request is successful, the bridge will reply back with a `200 OK` HTTP code, and a JSON payload with the consumer ID (`instance_id`) and base URL (`base_uri`).

    It should look similar to the following example of the output:

    ```json
    {
       "instance_id":"my-consumer",
       "base_uri":"http://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/consumers/my-group/instances/my-consumer"
    }
    ```

    ### Subscribing to a topic

    The most common way for a Kafka consumer to get messages from a topic is to subscribe to that topic as part of a consumer group and have partitions assigned automatically.

    Using the HTTP bridge, subscription is possible through an HTTP POST to the `/consumers/{groupid}/instances/{name}/subscription` endpoint, providing a list of topics to subscribe to or a topic pattern in a JSON formatted payload.

    Subscribe your `my-consumer` consumer to the `my-topic` topic with the following command:

    ```
    curl -i -k -X POST https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/consumers/my-group/instances/my-consumer/subscription -H 'content-type: application/vnd.kafka.v2+json' -d '{"topics": ["my-topic"]}'
    ```

    >This will return a `204 OK` HTTP code with an empty body.

    The `topics` array can contain a single topic (as shown here) or multiple topics. If you want to subscribe the consumer to multiple topics that match a regular expression, you can use the `topic_pattern` string instead of the `topics` array.

    After subscribing a Kafka Bridge consumer to topics, you can retrieve messages from the consumer.
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: intermediate
  timelimit: 225
- slug: step4
  id: rdkuij0su51v
  type: challenge
  title: Consuming messages
  assignment: |
    Finally, retrieve the latest messages from the Kafka Bridge consumer by requesting data from the `/consumers/{groupid}/instances/{name}/records` endpoint.

    ### Retrieving the latest messages from a Kafka Bridge consumer

    Using a HTTP GET method against the records endpoint performs a _poll_ for retrieving messages from the already subscribed topics. The first poll operation after the subscription doesn’t always return records. because it just starts the join operation of the consumer to the group, and the rebalancing in order to get partitions assigned. Doing the next poll returns the messages if there are any in the topic.

    Submit a `GET` request to the `records` endpoint:

    ```
    curl -s -k -X GET https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/consumers/my-group/instances/my-consumer/records -H 'accept: application/vnd.kafka.json.v2+json' | jq
    ```

    Repeat step a couple more times until you retrieve all messages from the Kafka Bridge consumer.

    ```
    curl -s -k -X GET https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/consumers/my-group/instances/my-consumer/records -H 'accept: application/vnd.kafka.json.v2+json' | jq
    ```

    The Kafka Bridge returns an array of messages — describing the topic name, key, value, partition, and offset — in the response body, along with a `200` code. Messages are retrieved from the latest offset by default.

    You should get an output similar to the following:

    ```js
    [  {
        "topic": "my-topic",
        "key": "key-1",
        "value": "sales-lead-0001",
        "partition": 0,
        "offset": 0
      },
      {
        "topic": "my-topic",
        "key": "key-2",
        "value": "sales-lead-0002",
        "partition": 0,
        "offset": 1
      }
    ]
    ```

    ### Committing offsets to the log

    Next, use the `/consumers/{groupid}/instances/{name}/offsets` endpoint to manually commit offsets to the log for all messages received by the Kafka Bridge consumer. This is required because the Kafka Bridge consumer that you created earlier was configured with the `enable.auto.commit` setting as false.

    Commit offsets to the log for `my-consumer`:

    ```
    curl -i -k -X POST https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/consumers/my-group/instances/my-consumer/offsets
    ```

    >Because no request body is submitted, offsets are committed for all the records that have been received by the consumer. Alternatively, the request body can contain an array (OffsetCommitSeekList) that specifies the topics and partitions that you want to commit offsets for.

    If the request is successful, the Kafka Bridge returns a `204` code only.

    Congratulations! You were able to deploy the AMQ Streams Kafka Bridge and connect to a Kafka cluster using HTTP. You sent some message to an example topic and then created a consumer to retrieve messages. Finally you committed the offsets the messages.
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: intermediate
  timelimit: 225
checksum: "7067126555879695264"
