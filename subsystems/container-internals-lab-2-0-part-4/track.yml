challenges:
- assignment: "If you just do a quick Google search, you will find tons of architectural\
    \ drawings which depict things the wrong way or only tell part of the story. This\
    \ leads the innocent viewer to come to the wrong conclusion about containers.\
    \ One might suspect that even the makers of these drawings have the wrong conclusion\
    \ about how containers work and hence propogate bad information. So, forget everything\
    \ you think you know.\n\n![Containers Are Linux](https://katacoda.com/openshift/assets/subsystems/container-internals-lab-2-0-part-4/01-google-wrong.png)\n\
    \nHow do people get it wrong? In two main ways:\n \nFirst, most of the architectural\
    \ drawings above show the docker daemon as a wide blue box stretched out over\
    \ the [Container Host](https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction/#h.8tyd9p17othl).\
    \ The [Containers](https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction/#h.j2uq93kgxe0e)\
    \ are shown as if they are running on top of the docker daemon. This is incorrect\
    \ - [containers don't run on docker](http://crunchtools.com/containers-dont-run-on-docker/).\
    \ The docker engine is an example of a general purpose [Container Engine](https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction/#h.6yt1ex5wfo3l).\
    \ Humans talk to container engines and container engines talk to the kernel -\
    \ the containers are actually created and run by the Linux kernel. Even when drawings\
    \ do actually show the right architecture between the container engine and the\
    \ kernel, they never show containers running side by side:\n\n![Containers Are\
    \ Linux](https://katacoda.com/openshift/assets/subsystems/container-internals-lab-2-0-part-4/01-not-on-docker.png)\n\
    \nSecond, when drawings show containers are Linux processes, they never show the\
    \ container engine side by side. This leads people to never think about these\
    \ two things together, hence users are left confused with only part of the story:\n\
    \n![Containers Are Linux](https://katacoda.com/openshift/assets/subsystems/container-internals-lab-2-0-part-4/01-not-the-whole-story.png)\n\
    \ \nFor this lab, let\u2019s start from scratch. In the terminal, let's start\
    \ with a simple experiment - start three containers which will all run the top\
    \ command:\n\n\n```\ndocker run -td registry.access.redhat.com/ubi7/ubi top\n\
    ```\n\n```\ndocker run -td registry.access.redhat.com/ubi7/ubi top\n```\n\n```\n\
    podman run -td registry.access.redhat.com/ubi7/ubi top\n```\n\nNow, let's inspect\
    \ the process table of the underlying host:\n\n```\nps -efZ | grep -v grep | grep\
    \ \" top\"\n```\n\nNotice that we started each of the ``top`` commands in containers.\
    \ We started two with docker, and one with podman, but they are still just a regular\
    \ process which can be viewed with the trusty old ``ps`` command. That's because\
    \ containerized processes are just [fancy Linux processes](http://sdtimes.com/guest-view-containers-really-just-fancy-files-fancy-processes/)\
    \ with extra isolation from normal Linux processes. Hack around a bit, and notice\
    \ that the docker daemon runs side by side with the containerized processes. A\
    \ simplified drawing should really look something like this:\n\n![Containers Are\
    \ Linux](https://katacoda.com/openshift/assets/subsystems/container-internals-lab-2-0-part-4/01-single-node-toolchain.png)\
    \ \n\nIn the kernel, there is no single data structure which represents what a\
    \ container is. This has been debated back and forth for years - some people think\
    \ there should be, others think there shouldn't. The current Linux kernel community\
    \ philosophy is that the Linux kernel should provide a bunch of different technologies,\
    \ ranging from experimental to very mature, enabling users to mix these technologies\
    \ together in creative, new ways. And, that's exactly what a container engine\
    \ (Docker, Podman, CRI-O, etc) does - it leverages kernel technologies to create,\
    \ what we humans call, containers. The concept of a container is a user construct,\
    \ not a kernel construct. This is a common pattern in Linux and Unix - this split\
    \ between lower level (kernel) and higher level (userspace) technologies allows\
    \ kernel developers to focus on enabling technologies, while users experiment\
    \ with them and find out what works well.\n\n![Containers Are Linux](https://katacoda.com/openshift/assets/subsystems/container-internals-lab-2-0-part-4/01-kernel-engine.png)\n\
    \ \nThe Linux kernel only has a single major data structure that tracks processes\
    \ - the process id table. The ``ps`` command dumps the contents of this data structure.\
    \ But, this is not the total definition of a container - the container engine\
    \ tracks which kernel isolation technologies are used, and even what data volumes\
    \ are mounted. This information can be thought of as metadata which provides a\
    \ definition for what we humans call a container. We will dig deeper into the\
    \ technical underpinnings, but for now, understand that containerized processes\
    \ are regular Linux processes which are isolated using kernel technologies like\
    \ namespaces, selinux and cgroups. This is sometimes described as \"sand boxing\"\
    \ or \"isolation\" or an \"illusion\" of virtualization.\n\nIn the end though,\
    \ containerized processes are just regular Linux processes. All processes live\
    \ side by side, whether they are regular Linux processes, long lived daemons,\
    \ batch jobs, interactive commands which you run manually, or containerized processes.\
    \ All of these processes make requests to the Linux kernel for protected resources\
    \ like memory, RAM, TCP sockets, etc. We will explore this deeper in later labs,\
    \ but for now, commit this to memory...\n"
  difficulty: expert
  notes:
  - contents: '## Background

      This lab focuses on understanding how a particular host actually runs the container
      images - from the deep internals of how containerized processes interact with
      the Linux kernel, to the docker daemon and how it receives REST API calls and
      translates them into system calls to tell the Linux kernel to create new containerized
      processes.


      By the end of this lab you should be able to:

      - Understand the basic interactions of the major daemons and APIs in a typical
      container environment

      - Internalize the function of system calls and kernel namespaces

      - Understand how SELinux and sVirt secures containers

      - Command a conceptual understanding of how cgroups limit containers

      - Use SECCOMP to limit the system calls a container can make

      - Have a basic understanding of container storage and how it compares to normal
      Linux storage concepts

      - Gain a basic understanding of container networking and namespaces

      - Troubleshoot a basic Open vSwitch setup with Kubernetes/OpenShift


      ## Outline

      - Daemons & APIs: Docker, Kubernetes Master, Node, and their interaction with
      the Linux kernel

      - System Calls & Kernel Namespaces: How they work inside of a container

      - SELinux & sVirt: Dynamically generated contexts to protect your containers

      - Cgroups: Dynamically created with container instantiation

      - SECCOMP: Limiting how a containerized process can interact with the kernel

      - Storage: How containers get local, copy on write storage

      - Pod Networking: How individual containers and pods connect to the network

      - Cluster Networking: How clusters of hosts manage the connections of containerized
      processes


      ## Other Material

      This video will give you a background to all of the concepts in this lab

      - [Presentation](https://goo.gl/UNnLkH)

      - [Lab GitHub Repository](https://github.com/openshift-labs/learn-katacoda)


      ## Start Scenario

      Once you have watched the background video or went throught the presentation,
      continue to the exercises

      '
    type: text
  slug: 01-engines-kernel
  tabs:
  - hostname: crc
    title: CLI
    type: terminal
  - hostname: crc
    path: /
    port: 30001
    title: Web Console
    type: service
  - hostname: crc
    path: /root
    title: Visual Editor
    type: code
  timelimit: 360
  title: Container Engines & The Linux Kernel
  type: challenge
- assignment: 'In this step, we are going to investigate the basic construction of
    a container. The general contruction of a container is similar with almost all
    OCI compliant container engines:


    1. Pull/expand/mount image

    2. Create OCI compliant spec file

    3. Call runc with the spec file



    ## Pull/Expand/Mount Image


    Let''s use podman to create a container from scratch. Podman makes it easy to
    break down each step of the container construction for learning purposes. First,
    let''s pull the image, expand it, and create a new overlay filesystem layer as
    the read/write root filesystem for the container. To do this, we will use a specially
    constructed container image which lets us break down the steps instead of starting
    all at once:


    ```

    podman create -dt --name on-off-container -v /mnt:/mnt:Z quay.io/fatherlinux/on-off-container

    ```


    After running the above command we have storage created. Notice under the STATUS
    column, the container is the "Created" state. This is not a running container,
    just the first step in creation has been executed:


    ```

    podman ps -a

    ```


    Try to look at the storage with the mount command (hint, you won''t be able to
    find it):


    ```

    mount | grep -v docker | grep merged

    ```


    Hopefully you didn''t look for too long because you can''t see it with the mount
    command. That''s because this storage has been "mounted" in what''s called a mount
    namespace. You can only see the mount from inside the container. To see the mount
    from outside the container, podman has a cool feature called podman-mount. This
    command will return the path of a directory which you can poke around in:


    ```

    podman mount on-off-container

    ```


    The directory you get back is a system level mount point into the overlay filesystem
    which is used by the container. You can literally change anything in the container''s
    filesystem now. Run a few commands to poke around:


    ```

    mount | grep -v docker | grep merged

    ```


    ```

    ls $(podman mount on-off-container)

    ```


    ```

    touch $(podman mount on-off-container)/test

    ```


    ```

    ls $(podman mount on-off-container)

    ```


    See the test file there. You will see this file in our container later when we
    start it and create a shell inside of it. Let''s move on to the spec file...


    ## Create Spec File


    At this point the container image has been cached locally and mounted, but we
    don''t actually have a spec file for runc yet. Creating a spec file from hand
    is quite tedious because they are made up of complex JSON with a lot of different
    options (governed by the OCI runtime spec). Luckily for us, the container engine
    will create one for us. This exact same spec file can be used by any OCI compliant
    runtime can consume it (runc, crun, katacontainers, gvisor, etc). Let''s run some
    experiments to show when it''s created. First let''s inspect the place where it
    should be:


    ```

    cat /var/lib/containers/storage/overlay-containers/$(podman ps -l -q --no-trunc)/userdata/config.json|jq
    .

    ```


    The above command errors out because the container engine hasn''t created the
    config.json file yet. We will initiate the creation of this file by using podman
    combined with a specially constructed container image:


    ```

    podman start on-off-container

    ```


    Now, the config.json file has been created. Inspect it for a while. Notice that
    there are options in there that are strikingly similar to the command line options
    of podman. The spec file really highlights the API:


    ```

    cat /var/lib/containers/storage/overlay-containers/$(podman ps -l -q --no-trunc)/userdata/config.json|jq
    .

    ```


    Podman has not started a container, just created the config.json and immediately
    exited. Notice under the STATUS column, that the container is now in the Exited
    state:


    ```

    podman ps -a

    ```


    In the next step, we will create a running container with runc...


    ## Call Runtime


    Now that we have storage and a config.json, let''s complete the circuit and create
    a containerized process with this config.json. We have constructed a container
    image, which only starts a process in the container if the /mnt/on file exists.
    Let''s create the file, and start the container again:


    ```

    touch /mnt/on

    ```


    ```

    podman start on-off-container

    ```


    When podman started the container this time, it fired up top. Notice under the
    STATUS column, that the container is now in the Up state::


    ```

    podman ps -a

    ```


    Now, let''s fire up a shell inside of our running container:


    ```

    podman exec -it on-off-container bash

    ```


    Now, look for the test file we created before we started the container:


    ```

    ls -alh

    ```


    The file is there like we would expect. You have just created a container in three
    basic steps. Did you know and understand that all of this was happening every
    time you ran a podman or docker command? Now, clean up your work:


    ```

    exit

    ```


    `podman kill -a

    podman rm -a`{{execute}}

    '
  difficulty: expert
  slug: 02-create-container
  tabs:
  - hostname: crc
    title: CLI
    type: terminal
  - hostname: crc
    path: /
    port: 30001
    title: Web Console
    type: service
  - hostname: crc
    path: /root
    title: Visual Editor
    type: code
  timelimit: 360
  title: Step by step creation of a container
  type: challenge
- assignment: 'The goal of this exercise is to gain a basic understanding of SELinux/sVirt.
    Run the following commands:


    `podman run -dt registry.access.redhat.com/ubi7/ubi sleep 10

    podman run -dt registry.access.redhat.com/ubi7/ubi sleep 10

    sleep 3

    ps -efZ | grep container_t | grep sleep`{{execute}}



    Example Output:


    ``system_u:system_r:container_t:s0:c228,c810 root 18682 18669  0 03:30 pts/0 00:00:00
    sleep 10

    system_u:system_r:container_t:s0:c184,c827 root 18797 18785  0 03:30 pts/0 00:00:00
    sleep 10``


    Notice that each container is labeled with a dynamically generated [Multi Level
    Security (MLS)](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/selinux_users_and_administrators_guide/mls)
    label. In the example above, the first container has an MLS label of c228,c810
    while the second has a label of c184,c827. Since each of these containers is started
    with a different MLS label, they are prevented from accessing each other''s memory,
    files, etc.


    SELinux doesn''t just label the processes, it must also label the files accessed
    by the process. Make a directory for data, and inspect the SELinux label on the
    directory. Notice the type is set to "user_tmp_t" but there are no MLS labels
    set:


    ```

    mkdir /tmp/selinux-test

    ```


    ```

    ls -alhZ /tmp/selinux-test/

    ```



    Example Output:


    ``drwxr-xr-x. root root system_u:object_r:container_file_t:s0:c177,c734 .

    drwxrwxrwt. root root system_u:object_r:tmp_t:s0       ..``



    Now, run the following command a few times and notice the MLS labels change every
    time. This is sVirt at work:


    ```

    podman run -t -v /tmp/selinux-test:/tmp/selinux-test:Z registry.access.redhat.com/ubi7/ubi
    ls -alhZ /tmp/selinux-test

    ```



    Finally, look at the MLS label set on the directory, it is always the same as
    the last container that was run. The :Z option auto-labels and bind mounts so
    that the container can access and change files on the mount point. This prevents
    any other process from accessing this data and is done transparently to the end
    user.


    ```

    ls -alhZ /tmp/selinux-test/

    ```

    '
  difficulty: expert
  slug: 03-selinux-svirt
  tabs:
  - hostname: crc
    title: CLI
    type: terminal
  - hostname: crc
    path: /
    port: 30001
    title: Web Console
    type: service
  - hostname: crc
    path: /root
    title: Visual Editor
    type: code
  timelimit: 360
  title: 'SELinux & sVirt: Dynamically generated contexts to protect your containers'
  type: challenge
- assignment: "The goal of this exercise is to gain a basic understanding of how containers\
    \ prevent using each other's reserved resources. The Linux kernel has a feature\
    \ called cgroups (abbreviated from control groups) which limits, accounts for,\
    \ and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of processes.\
    \ Normally, these control groups would be set up by a system administrator (with\
    \ cgexec), or configured with systemd (systemd-run --slice), but with a container\
    \ engine, this configuration is handled automatically.\n\nTo demonstrate, run\
    \ two separate containerized sleep processes: \n\n`podman run -dt registry.access.redhat.com/ubi7/ubi\
    \ sleep 10\npodman run -dt registry.access.redhat.com/ubi7/ubi sleep 10\nsleep\
    \ 3\nfor i in $(podman ps | grep sleep | awk '{print $1}' | grep [0-9]); do find\
    \ /sys/fs/cgroup/ | grep $i; done`{{execute}}\n\nNotice how each containerized\
    \ process is put into its own cgroup by the container engine. This is quite convenient,\
    \ similar to sVirt.\n\n"
  difficulty: expert
  slug: 04-cgroups
  tabs:
  - hostname: crc
    title: CLI
    type: terminal
  - hostname: crc
    path: /
    port: 30001
    title: Web Console
    type: service
  - hostname: crc
    path: /root
    title: Visual Editor
    type: code
  timelimit: 360
  title: 'Cgroups: Dynamically created with container instantiation'
  type: challenge
- assignment: "The goal of this exercise is to gain a basic understanding of SECCOMP.\
    \ Think of a SECCOMP as a firewall which can be configured to block certain system\
    \ calls.  While optional, and not configured by default, this can be a very powerful\
    \ tool to block misbehaved containers. Take a look at this sample:\n\n```\ncat\
    \ ~/labs/lab3-step5/chmod.json\n```\n\n\nNow, run a container with this profile\
    \ and test if it works. \n\n```\npodman run -it --security-opt seccomp=./labs/lab3-step5/chmod.json\
    \ registry.access.redhat.com/ubi7/ubi chmod 777 /etc/hosts\n```\n\nNotice how\
    \ the chmod system call is blocked.\n"
  difficulty: expert
  slug: 05-seccomp
  tabs:
  - hostname: crc
    title: CLI
    type: terminal
  - hostname: crc
    path: /
    port: 30001
    title: Web Console
    type: service
  - hostname: crc
    path: /root
    title: Visual Editor
    type: code
  timelimit: 360
  title: 'SECCOMP: Limiting how a containerized process can interact with the kernel'
  type: challenge
description: '## Background

  This lab focuses on understanding how a particular host actually runs the container
  images - from the deep internals of how containerized processes interact with the
  Linux kernel, to the docker daemon and how it receives REST API calls and translates
  them into system calls to tell the Linux kernel to create new containerized processes.


  By the end of this lab you should be able to:

  - Understand the basic interactions of the major daemons and APIs in a typical container
  environment

  - Internalize the function of system calls and kernel namespaces

  - Understand how SELinux and sVirt secures containers

  - Command a conceptual understanding of how cgroups limit containers

  - Use SECCOMP to limit the system calls a container can make

  - Have a basic understanding of container storage and how it compares to normal
  Linux storage concepts

  - Gain a basic understanding of container networking and namespaces

  - Troubleshoot a basic Open vSwitch setup with Kubernetes/OpenShift


  ## Outline

  - Daemons & APIs: Docker, Kubernetes Master, Node, and their interaction with the
  Linux kernel

  - System Calls & Kernel Namespaces: How they work inside of a container

  - SELinux & sVirt: Dynamically generated contexts to protect your containers

  - Cgroups: Dynamically created with container instantiation

  - SECCOMP: Limiting how a containerized process can interact with the kernel

  - Storage: How containers get local, copy on write storage

  - Pod Networking: How individual containers and pods connect to the network

  - Cluster Networking: How clusters of hosts manage the connections of containerized
  processes


  ## Other Material

  This video will give you a background to all of the concepts in this lab

  - [Presentation](https://goo.gl/UNnLkH)

  - [Lab GitHub Repository](https://github.com/openshift-labs/learn-katacoda)


  ## Start Scenario

  Once you have watched the background video or went throught the presentation, continue
  to the exercises

  '
developers:
- dahmed@redhat.com
- nvinto@redhat.com
- rjarvine@redhat.com
icon: https://logodix.com/logo/1910931.png
level: advanced
owner: openshift
private: false
published: true
skipping_enabled: false
slug: subsystems-container-internals-lab-2-0-part-4
tags:
- openshift
title: Container Hosts
type: track
