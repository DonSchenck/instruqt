slug: developing-with-quarkus-kafka
id: xvfhtif4zzks
type: track
title: Reactive Streaming with Quarkus and Kafka
description: |-
  In this exercise, you will use the Quarkus Kafka extension to build a streaming application using MicroProfile Reative
  Streams Messaging and [Apache Kafka](https://kafka.apache.org), a distributed streaming platform.

  ## What is Apache Kafka?

  Apache Kafka is a distributed streaming platform. A streaming platform has three key capabilities:

    - Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system.
    - Store streams of records in a fault-tolerant durable way.
    - Process streams of records as they occur.

  Kafka is generally used for two broad classes of applications:

    - Building real-time streaming data pipelines that reliably get data between systems or applications
    - Building real-time streaming applications that transform or react to the streams of data


  ### Other possibilities

  Learn more at [quarkus.io](https://quarkus.io), or just drive on and get hands-on!
icon: https://logodix.com/logo/1910931.png
level: beginner
tags:
- openshift
owner: openshift
developers:
- nvinto@redhat.com
- rjarvine@redhat.com
- dahmed@redhat.com
private: false
published: true
challenges:
- slug: 01-add-extension
  id: 71wepwvdiwly
  type: challenge
  title: Step 1
  notes:
  - type: text
    contents: |-
      In this exercise, you will use the Quarkus Kafka extension to build a streaming application using MicroProfile Reative
      Streams Messaging and [Apache Kafka](https://kafka.apache.org), a distributed streaming platform.

      ## What is Apache Kafka?

      Apache Kafka is a distributed streaming platform. A streaming platform has three key capabilities:

        - Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system.
        - Store streams of records in a fault-tolerant durable way.
        - Process streams of records as they occur.

      Kafka is generally used for two broad classes of applications:

        - Building real-time streaming data pipelines that reliably get data between systems or applications
        - Building real-time streaming applications that transform or react to the streams of data


      ### Other possibilities

      Learn more at [quarkus.io](https://quarkus.io), or just drive on and get hands-on!
  assignment: |
    ## Import the code

    Let's refresh the code we'll be using. Run the following command to clone the sample project:

    ```
    cd /root/projects && rm -rf rhoar-getting-started && git clone https://github.com/openshift-katacoda/rhoar-getting-started
    ```

    # Inspect Java runtime

    An appropriate Java runtime has been installed for you. Ensure you can use it by running this command:

    ```
    $JAVA_HOME/bin/java --version
    ```

    The command should report the version in use, for example (the versions and dates may be slightly different than the below example):

    ```console
    openjdk 11.0.10 2021-01-19
    OpenJDK Runtime Environment AdoptOpenJDK (build 11.0.10+9)
    OpenJDK 64-Bit Server VM AdoptOpenJDK (build 11.0.10+9, mixed mode)
    ```

    If the command fails, wait a few moments and try again (it is installed in a background process and make take a few moments depending on system load).

    # The Project

    You start with a basic Maven-based application with the usual `pom.xml` entries for a Quarkus app.

    We've also included a frontend HTML file at `src/main/resources/META-INF/resources/index.html` that will render our stream.

    # The Application You Will Build

    The app consists of 3 components that pass messages via Kafka and an in-memory stream, then uses SSE to push messages to
    the browser. It looks like:

    ![kafka](https://katacoda.com/openshift/assets/middleware/quarkus/kafkaarch.png)

    ## Add Extension

    Like other exercises, we’ll need another extension to integrate Quarkus with Kafka. Install it by clicking on the following command:

    `cd /root/projects/rhoar-getting-started/quarkus/kafka &&
      mvn quarkus:add-extension -Dextensions="smallrye-reactive-messaging-kafka"`{{execute}}

    > The first time you add the extension, new dependencies may be downloaded via maven. This should only happen once, after that things will go even faster.

    This will add the necessary entries in your `pom.xml` to bring in the Kafka extension. You'll see:

    ```xml
    <dependency>
        <groupId>io.quarkus</groupId>
        <artifactId>quarkus-smallrye-reactive-messaging-kafka</artifactId>
    </dependency>
    ```
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 128
- slug: 02-create-generator
  id: 7dmuf3kfv5im
  type: challenge
  title: Step 2
  assignment: |
    # Create name generator

    To start building the app, create a new Java class by clicking: `src/main/java/org/acme/people/stream/NameGenerator.java`.

    Next, click **Copy to Editor** to add the following code to this file:

    <pre class="file" data-filename="./src/main/java/org/acme/people/stream/NameGenerator.java" data-target="replace">
    package org.acme.people.stream;

    import io.smallrye.mutiny.Multi;

    import javax.enterprise.context.ApplicationScoped;
    import org.acme.people.utils.CuteNameGenerator;
    import org.eclipse.microprofile.reactive.messaging.Outgoing;

    import java.time.Duration;

    @ApplicationScoped
    public class NameGenerator {

        @Outgoing("generated-name")
        public Multi&lt;String&gt; generate() {
            return Multi.createFrom().ticks().every(Duration.ofSeconds(5))
                .onOverflow().drop()
                .map(tick -> CuteNameGenerator.generate());
        }

    }
    </pre>

    This simple method:

    * Instructs Reactive Messaging to dispatch the items from returned stream to `generated-name`
    * Returns a [Mutiny](https://smallrye.io/smallrye-mutiny/) `Multi` stream emitting a random name every 5 seconds

    The method returns a Reactive Stream. The generated items are sent to the stream named `generated-name`. This stream is
    mapped to Kafka using the `application.properties` file that we will soon create.
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 128
- slug: 03-create-converter
  id: 6dqrjcvrgvri
  type: challenge
  title: Step 3
  assignment: |
    # Create name converter

    The name converter reads the names from Kafka, and transforms them, adding a random (English) honorific to the beginning of the name.

    Create a new Java class by clicking: `src/main/java/org/acme/people/stream/NameConverter.java`.

    Next, click **Copy to Editor** to add the following code to this file:

    <pre class="file" data-filename="./src/main/java/org/acme/people/stream/NameConverter.java" data-target="replace">
    package org.acme.people.stream;

    import javax.enterprise.context.ApplicationScoped;
    import org.eclipse.microprofile.reactive.messaging.Incoming;
    import org.eclipse.microprofile.reactive.messaging.Outgoing;
    import io.smallrye.reactive.messaging.annotations.Broadcast;

    @ApplicationScoped
    public class NameConverter {

        private static final String[] honorifics = {"Mr.", "Mrs.", "Sir", "Madam", "Lord", "Lady", "Dr.", "Professor", "Vice-Chancellor", "Regent", "Provost", "Prefect"};

        @Incoming("names")
        @Outgoing("my-data-stream")
        @Broadcast
        public String process(String name) {
            String honorific = honorifics[(int)Math.floor(Math.random() * honorifics.length)];
            return honorific + " " + name;
        }
    }
    </pre>

    This simple method:

    * Consumes the items from the `names` topic using `@Incoming`
    * Adds an _honorific_ to the start of each name in the stream
    * Sends the resulting `@Outgoing` stream to all _subscribers_ (`@Broadcast`) of the in-memory `my-data-stream` stream

    The `process()` method is called for every Kafka record from the `names` topic (configured in the application
    configuration which we'll do later). Every result is sent to the my-data-stream in-memory stream.
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 128
- slug: 04-create-resource
  id: ylqcgloevdna
  type: challenge
  title: Step 4
  assignment: |
    # Create HTTP resource

    Finally, let’s bind our stream to a JAX-RS resource.

    Create a final Java class by clicking: `src/main/java/org/acme/people/stream/NameResource.java`.

    Next, click **Copy to Editor** to add the following code to this file:

    <pre class="file" data-filename="./src/main/java/org/acme/people/stream/NameResource.java" data-target="replace">
    package org.acme.people.stream;

    import org.eclipse.microprofile.reactive.messaging.Channel;

    import org.reactivestreams.Publisher;
    import javax.inject.Inject;
    import javax.ws.rs.GET;
    import javax.ws.rs.Path;
    import javax.ws.rs.Produces;
    import javax.ws.rs.core.MediaType;

    /**
     * A simple resource retrieving the in-memory "my-data-stream" and sending the items as server-sent events.
     */
    @Path("/names")
    public class NameResource {

        @Inject
        @Channel("my-data-stream") Publisher&lt;String&gt; names;

        @GET
        @Path("/stream")
        @Produces(MediaType.SERVER_SENT_EVENTS)
        public Publisher&lt;String&gt; stream() {
            return names;
        }
    }
    </pre>

    This method:

      - `@Inject`s the `my-data-stream` stream using the `@Channel` qualifier
      - Indicates that the content is sent (`@Produces`) using *Server Sent Events*
      - Returns the stream (Reactive Stream)

    The `process()` method is called for every Kafka record from the `names` topic (configured in the application
    configuration which we'll do later). Every result is sent to the my-data-stream in-memory stream.

    > In the `src/main/resources/META-INF/resources/index.html` page you'll find code
    > which will make a request to this `/names/stream` endpoint using standard JavaScript running in the browser and draw
    > the resulting names using the [D3.js library](https://d3js.org/). The JavaScript that makes this call looks like this
    > (do not copy this into anything\!):
    >
    > ```javascript
    > var source = new EventSource("/names/stream");
    >
    > source.onmessage = function (event) {
    >
    >     console.log("received new name: " + event.data);
    >     // process new name in event.data
    >     // ...
    >
    >     // update the display with the new name
    >     update();
    > };
    > ```
    >  This code:
    >
    >   - Uses your browser’s support for the `EventSource` API (part of the W3C SSE standard) to call the endpoint
    >
    >   - Each time a message is received via SSE, *react* to it by running this function
    >
    >   - Refresh the display using the D3.js library
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 128
- slug: 05-configure-app
  id: g0sxepira4ir
  type: challenge
  title: Step 5
  assignment: |
    # Configure the app

    We need to configure our app to define how our app can connect to Kafka. Click: `src/main/resources/application.properties` to open this file. This file contains Quarkus configuration and is empty. The names of the properties for the Kafka extension are structured as follows:

    `mp.messaging.[outgoing|incoming].{channel-name}.property=value`

      - The `channel-name` segment must match the value set in the `@Incoming` and `@Outgoing` annotation:

      - `generated-price` → sink in which we write the prices

      - `prices` → source in which we read the prices

    Click **Copy to Editor** to add the following values to the `application.properties` file:

    <pre class="file" data-filename="./src/main/resources/application.properties" data-target="replace">
    # Configure the Kafka sink (we write to it)
    mp.messaging.outgoing.generated-name.bootstrap.servers=names-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092
    mp.messaging.outgoing.generated-name.connector=smallrye-kafka
    mp.messaging.outgoing.generated-name.topic=names
    mp.messaging.outgoing.generated-name.value.serializer=org.apache.kafka.common.serialization.StringSerializer

    # Configure the Kafka source (we read from it)
    mp.messaging.incoming.names.bootstrap.servers=names-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092
    mp.messaging.incoming.names.connector=smallrye-kafka
    mp.messaging.incoming.names.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
    </pre>

    The hostnames used above refer to the in-cluster hostnames that resolve to our running Kafka cluster on OpenShift.

    More details about this configuration is available on the [Producer
    configuration](https://kafka.apache.org/documentation/#producerconfigs) and [Consumer
    configuration](https://kafka.apache.org/documentation/#consumerconfigs) section from the Kafka documentation.

    > **Note**
    >
    > What about `my-data-stream`? This is an in-memory stream, not connected to a message broker.

    # Compilation Test

    To make sure you've got a compilable app and code is in its proper place, let's test the build. Run this command to compile and package the app:

    ```
    mvn clean package
    ```
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 128
- slug: 06-deploy-kafka
  id: hrm8ifu679pm
  type: challenge
  title: Step 6
  assignment: |+
    # Deploying Kafka to OpenShift

    To deploy Kafka to OpenShift, you'll need to login.

    You should already be logged in as an admin user. Click this command to verify:

    `oc whoami`{{execute T2}}

    It should respond with `admin`.

    ## Access OpenShift Project

    [Projects](https://docs.openshift.com/container-platform/3.6/architecture/core_concepts/projects_and_users.html#projects)
    are a top level concept to help you organize your deployments. An
    OpenShift project allows a community of users (or a user) to organize and manage
    their content in isolation from other communities.

    For this scenario, let's create a project that you will use to house Kafka. Click:

    `oc new-project kafka --display-name="Apache Kafka"`{{execute T2}}

    ## Deploy Kafka Operator

    To deploy Kafka, we'll use the _Strimzi_ Operator. Strimzi is an open source project that provides an easy way to run an Apache Kafka cluster on Kubernetes in various deployment configurations.

    First, click this command to deploy the Operator to our new `kafka` namespace:

    `oc create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka`{{execute T2}}

    Wait for the Operator to be deployed by running this command:

    `oc rollout status -w deployment/strimzi-cluster-operator`{{execute T2}}

    > If this command seems to be taking a long time, just CTRL-C it and run it again. It make take time to install Kafka depending on system load.

    You should eventually see:

    ```console
    deployment "strimzi-cluster-operator" successfully rolled out
    ```

    ## Deploy Kafka Cluster

    Before deploying the Kafka cluster, make sure you are on the project folder _projects/rhoar-getting-started/quarkus/kafka/_ by executing this command:

    `cd /root/projects/rhoar-getting-started/quarkus/kafka`{{execute T2}}

    Next, create a new `Kafka` object within Kubernetes that the operator is waiting for. Click this command to create it:

    `oc apply -f src/main/kubernetes/kafka-names-cluster.yaml`{{execute T2}}

    This command creates a simple Kafka object:

    ```yaml
    apiVersion: kafka.strimzi.io/v1beta1
    kind: Kafka
    metadata:
      name: names-cluster
    spec:
      kafka:
        replicas: 3
        listeners:
          ...
        config:
          ...
        storage:
          type: ephemeral
      zookeeper:
        ...
    ```

    ## Deploy Kafka Topic

    We’ll need to create a _topic_ for our application to stream to and from. To do so, click the following command to create the _topic_ object:

    `oc apply -f src/main/kubernetes/kafka-names-topic.yaml`{{execute T2}}

    This creates our `names` topic that our code and Quarkus configuration references.

    ## Confirm deployment

    Look at the list of pods being spun up and look for the Kafka pods for our cluster:

    `oc get pods|grep names-cluster`{{execute T2}}

    You should see something like:

    ``` none
    names-cluster-entity-operator-6cbfffc465-jthb7   3/3     Running   0          45s
    names-cluster-kafka-0                            1/1     Running   0          99s
    names-cluster-kafka-1                            1/1     Running   0          99s
    names-cluster-kafka-2                            1/1     Running   0          99s
    names-cluster-zookeeper-0                        1/1     Running   0          2m31s
    names-cluster-zookeeper-1                        1/1     Running   0          2m31s
    names-cluster-zookeeper-2                        1/1     Running   0          2m31s
    ```
    If the pods are still spinning up (not all in the _Running_ state with `1/1` or `3/3` containers running), keep clicking the above command until you see 3 _kafka_ pods, 3 _zookeeper_ pods, and the single _entity operator_ pod.

    It will take around 2 minutes to get all the Kafka pods up and running.

  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 128
- slug: 07-deploy-and-test
  id: joral7dw0ew3
  type: challenge
  title: Step 7
  assignment: |
    ## Install OpenShift extension

    Quarkus offers the ability to automatically generate OpenShift resources based on sane default and user supplied configuration. The OpenShift extension is actually a wrapper extension that brings together the [kubernetes](https://quarkus.io/guides/deploying-to-kubernetes) and [container-image-s2i](https://quarkus.io/guides/container-image#s2i) extensions with sensible defaults so that it’s easier for the user to get started with Quarkus on OpenShift.

    Run the following command to add it to our project:

    `mvn quarkus:add-extension -Dextensions="openshift"`{{execute T2}}

    Click **Copy to Editor** to add the following values to the `application.properties` file:

    <pre class="file" data-filename="./src/main/resources/application.properties" data-target="append">
    # Configure the OpenShift extension options
    quarkus.kubernetes-client.trust-certs=true
    quarkus.container-image.build=true
    quarkus.kubernetes.deploy=true
    quarkus.kubernetes.deployment-target=openshift
    quarkus.openshift.expose=true
    quarkus.openshift.labels.app.openshift.io/runtime=quarkus
    </pre>

    For more details of the above options:

    * `quarkus.kubernetes-client.trust-certs=true` - We are using self-signed certs in this simple example, so this simply says to the extension to trust them.
    * `quarkus.container-image.build=true` - Instructs the extension to build a container image
    * `quarkus.kubernetes.deploy=true` - Instructs the extension to deploy to OpenShift after the container image is built
    * `quarkus.kubernetes.deployment-target=openshift` - Instructs the extension to generate and create the OpenShift resources (like `DeploymentConfig`s and `Service`s) after building the container
    * `quarkus.openshift.expose=true` - Instructs the extension to generate an OpenShift `Route`.
    * `quarkus.openshift.labels.app.openshift.io/runtime=quarkus` - Adds a nice-looking icon to the app when viewing the OpenShift Developer Toplogy

    ## Login to OpenShift

    We'll deploy our app as the `developer` user. Run the following command to login with the OpenShift CLI:

    `oc login -u developer -p developer`{{execute T2}}

    You should see

    ```
    Login successful.

    You don't have any projects. You can try to create a new project, by running

        oc new-project <projectname>
    ```

    ## Create project

    Create a new project into which we'll deploy the app:

    `oc new-project quarkus-kafka --display-name="Quarkus on Kafka"`{{execute T2}}

    ## Deploy application to OpenShift

    Now let's deploy the application itself. Run the following command which will build and deploy using the OpenShift extension:

    `mvn clean package`{{execute T2}}

    > **NOTE**: This command will take a minute or two, as it builds the app, pushes a container image, and finally deploys the container to OpenShift.

    The output should end with `BUILD SUCCESS`.

    Finally, make sure it's actually done rolling out:

    `oc rollout status -w dc/people`{{execute T2}}

    Wait for that command to report `replication controller "people-1" successfully rolled out` before continuing.

    You can see the app deployed in the [OpenShift Developer Toplogy](https://console-openshift-console-[[HOST_SUBDOMAIN]]-443-[[KATACODA_HOST]].environments.katacoda.com/topology/ns/quarkus-kafka):

    You'll need to login with the same credentials as before:

    * Username: `developer`
    * Password: `developer`

    ![topology](https://katacoda.com/openshift/assets/middleware/quarkus/peopletopology.png)

    And now we can access using `curl` once again to confirm the app is up:

    `curl http://people-quarkus-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/hello`{{execute T2}}

    So now our app is deployed to OpenShift. Finally, let's confirm our streaming Kafka app is working.

    [Open up the web UI](http://people-quarkus-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com). You should see a word cloud being updated every 5 seconds:

    > It takes a few seconds to establish the connection to Kafka. If you don’t see new names generated every 5 seconds reload the browser page to re-initialize the SSE stream.

    ![kafka](https://katacoda.com/openshift/assets/middleware/quarkus/wordcloud.png)

    # Open the solution in an IDE in the Cloud!
    Want to continue exploring this solution on your own in the cloud? You can use the free [Red Hat CodeReady Workspaces](https://developers.redhat.com/products/codeready-workspaces/overview) IDE running on the free [Red Hat Developer Sandbox](http://red.ht/dev-sandbox). [Click here](https://workspaces.openshift.com) to login or to register if you are a new user. This free service expires after 30 days, but you can always enable a new free 30-day subscription.

    Once logged in, [click here](https://workspaces.openshift.com/f?url=https://raw.githubusercontent.com/openshift-katacoda/rhoar-getting-started/solution/quarkus/kafka/devfile.yaml) to open the solution for this project in the cloud IDE. While loading, if it asks you to update or install any plugins, you can say no.

    # Fork the source code to your own GitHub!
    Want to experiment more with the solution code you just worked with? If so, you can fork the repository containing the solution to your own GitHub repository by clicking on the following command to execute it:

    `/root/projects/forkrepo.sh`{{execute T1}}
    - Make sure to follow the prompts. An error saying `Failed opening a web browser at https://github.com/login/device exit status 127` is expected.
    - [Click here](https://github.com/login/device) to open a new browser tab to GitHub and paste in the code you were presented with and you copied.
    - Once done with the GitHub authorization in the browser, close the browser tab and return to the console and press `Enter` to complete the authentication process.
    - If asked to clone the fork, press `n` and then `Enter`.
    - If asked to confirm logout, press `y` and the `Enter`.

       > **NOTE:** This process uses the [GitHub CLI](https://cli.github.com) to authenticate with GitHub. The learn.openshift.com site is not requesting nor will have access to your GitHub credentials.

    After completing these steps the `rhoar-getting-started` repo will be forked in your own GitHub account. On the `solution` branch in the repo, the `kafka` project inside the `quarkus` folder contains the completed solution for this scenario.

    ## Congratulations!

    This guide has shown how you can interact with Kafka using Quarkus. It utilizes MicroProfile Reactive Messaging to build
    data streaming applications.

    If you want to go further check the documentation of [SmallRye Reactive
    Messaging](https://smallrye.io/smallrye-reactive-messaging), the implementation used in Quarkus.
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 128
checksum: "5418263697046907489"
