slug: serverless-camel-kafka-connector
id: 3iss6yyk7w3a
type: track
title: Camel Kafka Connector Basics
description: |2

  This scenario will introduce [Camel Kafka Connector ](https://camel.apache.org/camel-kafka-connector/latest/index.html).

  ## What is Camel Kafka Connector?


  ![Logo](https://upload.wikimedia.org/wikipedia/commons/1/11/Apache_Camel_Logo.svg)


  ### Kafka Connector with a Camel twist -- connect to ALMOST anything

  Camel Kafka Connector enables you to use standard Camel components as Kafka Connect connectors. This widens the scope of possible integrations beyond the external systems supported by Kafka Connect connectors alone. Camel Kafka Connector works as an adapter that makes the popular Camel component ecosystem available in Kafka-based AMQ Streams on OpenShift.

  Camel Kafka Connector provides a user-friendly way to configure Camel components directly in the Kafka Connect framework. Using Camel Kafka Connector, you can leverage Camel components for integration with different systems by connecting to or from Camel Kafka sink or source connectors. You do not need to write any code, and can include the appropriate connector JARs in your Kafka Connect image and configure connector options using custom resources.


  ### AMQ Streams - Kubernetes-native Apache Kafka

  The Red Hat AMQ streams component is a massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. It offers a distributed backbone that allows microservices and other applications to share data with high throughput and low latency.

  As more applications move to Kubernetes and Red Hat OpenShift , it is increasingly important to be able to run the communication infrastructure on the same platform. Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging technologies such as Kafka. The AMQ streams component makes running and managing Apache Kafka OpenShift native through the use of powerful operators that simplify the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift.
icon: https://logodix.com/logo/1910931.png
level: beginner
tags:
- openshift
owner: openshift
developers:
- nvinto@redhat.com
- rjarvine@redhat.com
- dahmed@redhat.com
private: false
published: true
challenges:
- slug: step1
  id: zlfqmxoxbvni
  type: challenge
  title: Step 1
  notes:
  - type: text
    contents: |2

      This scenario will introduce [Camel Kafka Connector ](https://camel.apache.org/camel-kafka-connector/latest/index.html).

      ## What is Camel Kafka Connector?


      ![Logo](https://upload.wikimedia.org/wikipedia/commons/1/11/Apache_Camel_Logo.svg)


      ### Kafka Connector with a Camel twist -- connect to ALMOST anything

      Camel Kafka Connector enables you to use standard Camel components as Kafka Connect connectors. This widens the scope of possible integrations beyond the external systems supported by Kafka Connect connectors alone. Camel Kafka Connector works as an adapter that makes the popular Camel component ecosystem available in Kafka-based AMQ Streams on OpenShift.

      Camel Kafka Connector provides a user-friendly way to configure Camel components directly in the Kafka Connect framework. Using Camel Kafka Connector, you can leverage Camel components for integration with different systems by connecting to or from Camel Kafka sink or source connectors. You do not need to write any code, and can include the appropriate connector JARs in your Kafka Connect image and configure connector options using custom resources.


      ### AMQ Streams - Kubernetes-native Apache Kafka

      The Red Hat AMQ streams component is a massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. It offers a distributed backbone that allows microservices and other applications to share data with high throughput and low latency.

      As more applications move to Kubernetes and Red Hat OpenShift , it is increasingly important to be able to run the communication infrastructure on the same platform. Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging technologies such as Kafka. The AMQ streams component makes running and managing Apache Kafka OpenShift native through the use of powerful operators that simplify the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift.
  assignment: |
    In order to run this tutorial, you will need access to an OpenShift environment.
    And we also have access to Kafka cluster in the environment. Let's setup the fundamentals.

    This tutorial is an simple *Managed File Transfer* scenario, where a laboratory uploads medical reports to an online object store, and we will need to transfer the file from the cloud object store to our local FTP server in order for the legacy system to consume.

    ![overview](https://katacoda.com/openshift/assets/middleware/middleware-camelk/camel-kafka-connector/camel-kafka-step01-overview.png)

    ## Logging in to the Cluster via CLI

    Before creating any applications, login as admin. This will be required if you want to log in to the web console and
    use it.

    To login to the OpenShift cluster from the _Terminal_ run:

    ```
    oc login -u admin -p admin
    ```

    This will log you in using the credentials:

    * **Username:** ``admin``
    * **Password:** ``admin``

    Use the same credentials to log into the web console.

    (OPTIONAL): Click the [Console](https://console-openshift-console-[[HOST_SUBDOMAIN]]-443-[[KATACODA_HOST]].environments.katacoda.com) tab to open the dashboard.

    ## Create a new namespace for this tutorial

    To create a new project called ``camel-kafka`` run the command:

    ```
    oc project camel-kafka
    ```

    ## Start up the Kafka Cluster and the Kafka Connect

    AMQ Streams simplifies the process of running Apache Kafka in an Openshift cluster.
    Now let's deploy a Kafka broker cluster, the AMQ Streams Operator is already subscribed in the cluster,
    simply create the cluster by defining the resource definition:

    ```
    oc create -f strimzi/kafka-cluster.yaml
    ```

    Below indicates AMQ Streams has accept the configuration, and now starting to process.
    ```
    kafka.kafka.strimzi.io/my-cluster created
    ```

    If everything goes right, you will be able to see the pods initiated in the namespace:
    ```
    oc get pod -w
    ```

    ```
    amq-streams-cluster-operator-v1.5.3-7bbf5cdfdc-4kq7q   1/1     Running   0          3m15s
    my-cluster-entity-operator-59cf586599-vdmk5            0/3     Running   0          7s
    my-cluster-kafka-0                                     2/2     Running   0          40s
    my-cluster-kafka-1                                     2/2     Running   0          40s
    my-cluster-kafka-2                                     2/2     Running   0          40s
    my-cluster-zookeeper-0                                 1/1     Running   0          71s
    my-cluster-zookeeper-1                                 1/1     Running   0          71s
    my-cluster-zookeeper-2                                 1/1     Running   0          71s
    ```
    Ctrl+C to exit the mode.
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 300
- slug: step2
  id: pgrjldvgi8yu
  type: challenge
  title: Step 2
  assignment: |
    ## Setup the Minio generic object datastore

    Lets start Minio, it provide a S3 compatible protocol for storing the objects.
    To create the minio backend, just apply the provided file:

    ```
    oc apply -f minio/minio.yaml
    ```

    And now, you have a working generic object datastore.


    ## Setup the SFTP Server for the file store

    Run the following script to start up the SFTP Server in the system:

    ```
    bash sftp/sftp.sh
    ```

    If everything goes as planned, you should see the following message:
    ```
    clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: "default"
    --> Found container image d87b9ff (10 hours old) from Docker Hub for "atmoz/sftp"

        * An image stream tag will be created as "ftpserver:latest" that will track this image
        * This image will be deployed in deployment config "ftpserver"
        * Port 22/tcp will be load balanced by service "ftpserver"
          * Other containers can access this service through the hostname "ftpserver"
        * WARNING: Image "atmoz/sftp" runs as the 'root' user which may not be permitted by your cluster administrator

    --> Creating resources ...
        imagestream.image.openshift.io "ftpserver" created
        deploymentconfig.apps.openshift.io "ftpserver" created
        service "ftpserver" created
    --> Success
        Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
         'oc expose service/ftpserver'
        Run 'oc status' to view your app.
    secret/ftp-users created
    secret/sftp-ssh-key created
    info: Generated volume name: volume-dpt6b
    deploymentconfig.apps.openshift.io/ftpserver volume updated
    info: Generated volume name: volume-mrld4
    deploymentconfig.apps.openshift.io/ftpserver volume updated
    ```

    Take note at the secret/sftp-ssh-key, we will be using this secret for configuring our sink connector.

    ```
    oc get secret/sftp-ssh-key
    ```

    Waiting until the _RUNNING_ status

    ```
    oc get pod -w | grep ftpserver
    ```

    Hit Ctrl+C to exit the mode.

    Take a look at the folder, and it should be empty at the moment. This is where we want our file files to be at.

    ```
    oc exec -i `oc get pod -l deploymentconfig=ftpserver -o=jsonpath='{.items[0].metadata.name}'` -- ls /home/foo/upload/
    ```


    ## Setup Source2Image

    For the Kafka Connect image, we will need the connector libraries to run.
    Source2Image will be used to setup and build the connectors.Go to the text editor on the right, under the folder /root/camel-kafka. Right click on the directory and choose New -> File and name it `kafka-connect-s2i.yaml`.

    Paste the following code into the application.

    <pre class="file" data-filename="kafka-connect-s2i.yaml" data-target="replace">
    apiVersion: kafka.strimzi.io/v1beta1
    kind: KafkaConnectS2I
    metadata:
     name: my-connect-cluster
    spec:
      bootstrapServers: 'my-cluster-kafka-bootstrap:9093'
      config:
        config.storage.replication.factor: 1
        config.storage.topic: connect-cluster-configs
        group.id: connect-cluster
        offset.storage.replication.factor: 1
        offset.storage.topic: connect-cluster-offsets
        status.storage.replication.factor: 1
        status.storage.topic: connect-cluster-status
      externalConfiguration:
        volumes:
          - name: sftp-ssh-key
            secret:
              secretName: sftp-ssh-key
      replicas: 1
      tls:
        trustedCertificates:
          - certificate: ca.crt
            secretName: my-cluster-cluster-ca-cert
      version: 2.5.0
    </pre>

    Notice here it connects to the Kafka cluster that we initiated in the previous step, and also points to the secret in the SFTP server (we will need this later) as an external configuration. Go ahead execute the command to setup the KafkaConnect Source2Image.

    ```
    oc apply -f camel-kafka/kafka-connect-s2i.yaml
    ```


    Enable Kafka Connectors to be able to instantiated via custom resource:
    ```
    oc annotate kafkaconnects2is my-connect-cluster strimzi.io/use-connector-resources=true
    ```
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 300
- slug: step3
  id: mdri2qdwxtlg
  type: challenge
  title: Step 3
  assignment: |2

    ## Add and build the Camel Kafka Connector binary to the connector

    Take a look at the plugin folder, you will find two folders named

    ```
    ls ./plugin/
    ```

    - camel-minio-kafka-connector
    - camel-sftp-kafka-connector

    They are the libraries needed for the connectors. Now let's use Source2Image to add these connectors libraries to the existing images. We now need to build the connectors and add them to the image.

    ```
    oc start-build my-connect-cluster-connect --from-dir=./plugin/ --follow
    ```


    Wait for the rollout of the new image to finish and the replica set with the new connector to become ready. Once it is done, we can check that the connectors are available in our Kafka Connect cluster. Since AMQ Streams is running Kafka Connect in a distributed mode. We will use HTTP to interact with it. To check the available connector plugins, you can run the following command:

    ```
    oc exec -i `oc get pods --field-selector status.phase=Running -l strimzi.io/name=my-connect-cluster-connect -o=jsonpath='{.items[0].metadata.name}'` -- curl -s http://my-connect-cluster-connect-api:8083/connector-plugins
    ```


    You will find the two connectors that we are have added in the image.

    ```
    [
       ........
       {
          "class":"org.apache.camel.kafkaconnector.minio.CamelMinioSinkConnector",
          "type":"sink",
          "version":"0.5.0"
       },
       {
          "class":"org.apache.camel.kafkaconnector.minio.CamelMinioSourceConnector",
          "type":"source",
          "version":"0.5.0"
       },
       {
          "class":"org.apache.camel.kafkaconnector.sftp.CamelSftpSinkConnector",
          "type":"sink",
          "version":"0.6.0-SNAPSHOT"
       },
       {
          "class":"org.apache.camel.kafkaconnector.sftp.CamelSftpSourceConnector",
          "type":"source",
          "version":"0.6.0-SNAPSHOT"
       }
       .........
    ]
    ```

    ## Create Source Connector Instance

    Now we can create an instance connector that reads files from the AWS S3 liked object store (Minio). Go to the text editor on the right, under the folder /root/camel-kafka. Right click on the directory and choose New -> File and name it `minio-connector.yaml`.

    Paste the following code into the application.

    <pre class="file" data-filename="minio-connector.yaml" data-target="replace">
    apiVersion: kafka.strimzi.io/v1alpha1
    kind: KafkaConnector
    metadata:
      name: minio-source-connector
      labels:
        strimzi.io/cluster: my-connect-cluster
    spec:
      class: org.apache.camel.kafkaconnector.minio.CamelMinioSourceConnector
      tasksMax: 1
      config:
        key.converter: org.apache.kafka.connect.storage.StringConverter
        value.converter: org.apache.kafka.connect.storage.StringConverter
        topics: demo-topic
        camel.source.path.bucketName: camel-kafka
        camel.source.endpoint.initialDelay: 20000
        camel.source.endpoint.endpoint: http://minio:9000
        camel.component.minio.accessKey: minio
        camel.component.minio.secretKey: minio123
        camel.component.minio.operation: getObject
    </pre>

    This points the connector to listen to the camel-kafka in the Minio object store, when detected content from the Minio source, the connector will capture it and place it in the Kafka topic _*demo-topic*_ and ready to be subscribed by the consumers. Execute following command to create the source connector.

    ```
    oc create -f camel-kafka/minio-connector.yaml
    ```

    Now, let's go ahead and place a file in the object store. This is going to copy the healthcare related files that we have prepared into the object store.

    ```
    oc rsync ./file/ `oc get pod -l app=minio -o=jsonpath='{.items[0].metadata.name}'`:/data/camel-kafka
    ```

    Note the name of the two files.

    ```
    WARNING: cannot use rsync: rsync not available in container
    sample01.json
    ```

    You will find the file in the Kafka Topic _*demo-topic*_

    ```
    oc exec -i `oc get pods --field-selector status.phase=Running -l strimzi.io/name=my-connect-cluster-connect -o=jsonpath='{.items[0].metadata.name}'` -- bin/kafka-console-consumer.sh --topic demo-topic --from-beginning --bootstrap-server my-cluster-kafka-bootstrap:9092
    ```


    With content starting like following:

    ```
    OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
    {
      "resourceType": "Observation",
      "id": "ekg",
      "text": {
        "status": "generated",
        "div": "<div xmlns=\"http://www.w3.org/1999/xhtml\"><p><b>Generated Narrative with Details</b></p><p><b>id</b>: ekg</p><p><b>status</b>: final</p><p><b>category</b>: Procedure <span>(Details : {http://terminology.hl7.org/CodeSystem/observation-category code 'procedure' = 'Procedure', given as 'Procedure'})</span></p><p><b>code</b>: MDC_ECG_ELEC_POTL <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131328' = '131328', given as 'MDC_ECG_ELEC_POTL'})</span></p><p><b>subject</b>: <a>P. van de Heuvel</a></p><p><b>effective</b>: 19/02/2015 9:30:35 AM</p><p><b>performer</b>: <a>A. Langeveld</a></p><p><b>device</b>: 12 lead EKG Device Metric</p><blockquote><p><b>component</b></p><p><b>code</b>: MDC_ECG_ELEC_POTL_I <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131329' = '131329', given as 'MDC_ECG_ELEC_POTL_I'})</span></p><p><b>value</b>: Origin: (system = '[not stated]' code null = 'null'), Period: 10, Factor: 1.612, Lower: -3300, Upper: 3300, Dimensions: 1, Data: 2041 2043 2037 2047 2060 2062 2051 2023 2014 2027 2034 2033 2040 2047 2047 2053 2058 2064 2059 2063 2061 2052 2053 2038 1966 1885 1884 2009 2129 2166 2137 2102 2086 2077 2067 2067 2060 2059 2062 2062 2060 2057 2045 2047 2057 2054 2042 2029 2027 2018 2007 1995 2001 2012 2024 2039 2068 2092 2111 2125 2131 2148 2137 2138 2128 2128 2115 2099 2097 2096 2101 2101 2091 2073 2076 2077 2084 2081 2088 2092 2070 2069 2074 2077 2075 2068 2064 2060 2062 2074 2075 2074 2075 2063 2058 2058 2064 2064 2070 2074 2067 2060 2062 2063 2061 2059 2048 2052 2049 2048 2051 2059 2059 2066 2077 2073</p></blockquote><blockquote><p><b>component</b></p><p><b>code</b>: MDC_ECG_ELEC_POTL_II <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131330' = '131330', given as 'MDC_ECG_ELEC_POTL_II'})</span></p><p><b>value</b>: Origin: (system = '[not stated]' code null = 'null'), Period: 10, Factor: 1.612, Lower: -3300, Upper: 3300, Dimensions: 1, Data: 2041 2043 2037 2047 2060 2062 2051 2023 2014 2027 2034 2033 2040 2047 2047 2053 2058 2064 2059 2063 2061 2052 2053 2038 1966 1885 1884 2009 2129 2166 2137 2102 2086 2077 2067 2067 2060 2059 2062 2062 2060 2057 2045 2047 2057 2054 2042 2029 2027 2018 2007 1995 2001 2012 2024 2039 2068 2092 2111 2125 2131 2148 2137 2138 2128 2128 2115 2099 2097 2096 2101 2101 2091 2073 2076 2077 2084 2081 2088 2092 2070 2069 2074 2077 2075 2068 2064 2060 2062 2074 2075 2074 2075 2063 2058 2058 2064 2064 2070 2074 2067 2060 2062 2063 2061 2059 2048 2052 2049 2048 2051 2059 2059 2066 2077 2073</p></blockquote><blockquote><p><b>component</b></p><p><b>code</b>: MDC_ECG_ELEC_POTL_III <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131389' = '131389', given as 'MDC_ECG_ELEC_POTL_III'})</span></p><p><b>value</b>: Origin: (system = '[not stated]' code null = 'null'), Period: 10, Factor: 1.612, Lower: -3300, Upper: 3300, Dimensions: 1, Data: 2041 2043 2037 2047 2060 2062 2051 2023 2014 2027 2034 2033 2040 2047 2047 2053 2058 2064 2059 2063 2061 2052 2053 2038 1966 1885 1884 2009 2129 2166 2137 2102 2086 2077 2067 2067 2060 2059 2062 2062 2060 2057 2045 2047 2057 2054 2042 2029 2027 2018 2007 1995 2001 2012 2024 2039 2068 2092 2111 2125 2131 2148 2137 2138 2128 2128 2115 2099 2097 2096 2101 2101 2091 2073 2076 2077 2084 2081 2088 2092 2070 2069 2074 2077 2075 2068 2064 2060 2062 2074 2075 2074 2075 2063 2058 2058 2064 2064 2070 2074 2067 2060 2062 2063 2061 2059 2048 2052 2049 2048 2051 2059 2059 2066 2077 2073</p></blockquote></div>"
      },
      "status": "final",
      "category": [
        {
          "coding": [
            {
       ............
    ```
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 300
- slug: step4
  id: e6cjtsujywxd
  type: challenge
  title: Step 4
  assignment: |
    ## Create Sink Connector Instance
    Finally, we need to read the file from the Kafka Topic, rename it into something our system understand and place it into the SFTP server.
    We need an instance of Camel Kafka connector that move files from the updated topic to sink -- SFTP server. Go to the text editor on the right, under the folder /root/camel-kafka. Right click on the directory and choose New -> File and name it `sftp-connector.yaml`.

    Paste the following code into the application.

    <pre class="file" data-filename="sftp-connector.yaml" data-target="replace">
    apiVersion: kafka.strimzi.io/v1alpha1
    kind: KafkaConnector
    metadata:
      name: sftp-source-connector
      labels:
        strimzi.io/cluster: my-connect-cluster
    spec:
      class: org.apache.camel.kafkaconnector.sftp.CamelSftpSinkConnector
      tasksMax: 1
      config:
        key.converter: org.apache.kafka.connect.storage.StringConverter
        value.converter: org.apache.kafka.connect.storage.StringConverter
        topics: demo-topic
        camel.sink.path.host: ftpserver
        camel.sink.path.port: 22
        camel.sink.path.directoryName: upload
        camel.sink.endpoint.fileName: ${date:now:yyyyMMddhhmmss}.json
        camel.sink.endpoint.username: foo
        camel.sink.endpoint.password: pass
        camel.sink.endpoint.useUserKnownHostsFile: false
        camel.sink.endpoint.privateKeyFile: /opt/kafka/external-configuration/sftp-ssh-key/demo_rsa
    </pre>

    This points the connector to the Kafka topic _*demo-topic*_ and write the incoming event data into the SFTP server under the upload folder as configured, with the name of the file changed to the yyyyMMddhhmmss format. Also points to the keyfile that we have configured earlier in the secrets.

    Run command to create the sink connector.

    ```
    oc create -f camel-kafka/sftp-connector.yaml
    ```

    Check if the file appears in the SFTP server.

    ```
    oc exec -i `oc get pod -l deploymentconfig=ftpserver -o=jsonpath='{.items[0].metadata.name}'` -- ls /home/foo/upload/
    ```

    You should be seeing the same file that was updloaded!
    ```
    oc exec -i `oc get pod -l deploymentconfig=ftpserver -o=jsonpath='{.items[0].metadata.name}'` -- find /home/foo/upload/ -type f -exec cat {} \;
    ```


    ```
    OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
    {
      "resourceType": "Observation",
      "id": "ekg",
      "text": {
        "status": "generated",
        "div": "<div xmlns=\"http://www.w3.org/1999/xhtml\"><p><b>Generated Narrative with Details</b></p><p><b>id</b>: ekg</p><p><b>status</b>: final</p><p><b>category</b>: Procedure <span>(Details : {http://terminology.hl7.org/CodeSystem/observation-category code 'procedure' = 'Procedure', given as 'Procedure'})</span></p><p><b>code</b>: MDC_ECG_ELEC_POTL <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131328' = '131328', given as 'MDC_ECG_ELEC_POTL'})</span></p><p><b>subject</b>: <a>P. van de Heuvel</a></p><p><b>effective</b>: 19/02/2015 9:30:35 AM</p><p><b>performer</b>: <a>A. Langeveld</a></p><p><b>device</b>: 12 lead EKG Device Metric</p><blockquote><p><b>component</b></p><p><b>code</b>: MDC_ECG_ELEC_POTL_I <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131329' = '131329', given as 'MDC_ECG_ELEC_POTL_I'})</span></p><p><b>value</b>: Origin: (system = '[not stated]' code null = 'null'), Period: 10, Factor: 1.612, Lower: -3300, Upper: 3300, Dimensions: 1, Data: 2041 2043 2037 2047 2060 2062 2051 2023 2014 2027 2034 2033 2040 2047 2047 2053 2058 2064 2059 2063 2061 2052 2053 2038 1966 1885 1884 2009 2129 2166 2137 2102 2086 2077 2067 2067 2060 2059 2062 2062 2060 2057 2045 2047 2057 2054 2042 2029 2027 2018 2007 1995 2001 2012 2024 2039 2068 2092 2111 2125 2131 2148 2137 2138 2128 2128 2115 2099 2097 2096 2101 2101 2091 2073 2076 2077 2084 2081 2088 2092 2070 2069 2074 2077 2075 2068 2064 2060 2062 2074 2075 2074 2075 2063 2058 2058 2064 2064 2070 2074 2067 2060 2062 2063 2061 2059 2048 2052 2049 2048 2051 2059 2059 2066 2077 2073</p></blockquote><blockquote><p><b>component</b></p><p><b>code</b>: MDC_ECG_ELEC_POTL_II <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131330' = '131330', given as 'MDC_ECG_ELEC_POTL_II'})</span></p><p><b>value</b>: Origin: (system = '[not stated]' code null = 'null'), Period: 10, Factor: 1.612, Lower: -3300, Upper: 3300, Dimensions: 1, Data: 2041 2043 2037 2047 2060 2062 2051 2023 2014 2027 2034 2033 2040 2047 2047 2053 2058 2064 2059 2063 2061 2052 2053 2038 1966 1885 1884 2009 2129 2166 2137 2102 2086 2077 2067 2067 2060 2059 2062 2062 2060 2057 2045 2047 2057 2054 2042 2029 2027 2018 2007 1995 2001 2012 2024 2039 2068 2092 2111 2125 2131 2148 2137 2138 2128 2128 2115 2099 2097 2096 2101 2101 2091 2073 2076 2077 2084 2081 2088 2092 2070 2069 2074 2077 2075 2068 2064 2060 2062 2074 2075 2074 2075 2063 2058 2058 2064 2064 2070 2074 2067 2060 2062 2063 2061 2059 2048 2052 2049 2048 2051 2059 2059 2066 2077 2073</p></blockquote><blockquote><p><b>component</b></p><p><b>code</b>: MDC_ECG_ELEC_POTL_III <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131389' = '131389', given as 'MDC_ECG_ELEC_POTL_III'})</span></p><p><b>value</b>: Origin: (system = '[not stated]' code null = 'null'), Period: 10, Factor: 1.612, Lower: -3300, Upper: 3300, Dimensions: 1, Data: 2041 2043 2037 2047 2060 2062 2051 2023 2014 2027 2034 2033 2040 2047 2047 2053 2058 2064 2059 2063 2061 2052 2053 2038 1966 1885 1884 2009 2129 2166 2137 2102 2086 2077 2067 2067 2060 2059 2062 2062 2060 2057 2045 2047 2057 2054 2042 2029 2027 2018 2007 1995 2001 2012 2024 2039 2068 2092 2111 2125 2131 2148 2137 2138 2128 2128 2115 2099 2097 2096 2101 2101 2091 2073 2076 2077 2084 2081 2088 2092 2070 2069 2074 2077 2075 2068 2064 2060 2062 2074 2075 2074 2075 2063 2058 2058 2064 2064 2070 2074 2067 2060 2062 2063 2061 2059 2048 2052 2049 2048 2051 2059 2059 2066 2077 2073</p></blockquote></div>"
      },
      "status": "final",
      "category": [
        {
          "coding": [
            {
       ............
    ```

    ![overview](https://katacoda.com/openshift/assets/middleware/middleware-camelk/camel-kafka-connector/camel-kafka-step01-overview.png)

    ## Congratulations

    In this scenario you got to play with Camel Kafka Connector. You have initiated an AMQ Streams (Kafka) cluster on openshift, created an object store, sftp server. Then started a Kafka Connect with your own build of image including the libraries needed. Then you created a source Camel Kafka connector that loads files from the _AWS2 S3_ like storage, place the file in the kafka topic, and created another Camel Kafka connector that listens to the input from the kafka topic and place the file in the SFTP server.
  tabs:
  - title: CLI
    type: terminal
    hostname: crc
  - title: Web Console
    type: service
    hostname: crc
    path: /
    port: 30001
  - title: Visual Editor
    type: code
    hostname: crc
    path: /root
  difficulty: basic
  timelimit: 300
checksum: "2257998432597175673"
